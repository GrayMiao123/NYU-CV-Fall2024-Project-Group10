@electronic{shell2007ieeetran,
  author = {Michael Shell},
  title  = {{IEEE}tran Homepage},
  url    = {http://www.michaelshell.org/tex/ieeetran/},
  year   = {2007}
}

@inbook{bul1964magnetic,
  author    = {B. K. Bul},
  title     = {Theory Principles and Design of Magnetic Circuits},
  publisher = {Energia Press},
  address   = {Moscow},
  year      = {1964},
  pages     = {464}
}

@inproceedings{finn1997protection,
  author    = {S. G. Finn and M. M{\'e}dard and R. A. Barry},
  title     = {A Novel Approach to Automatic Protection Switching
               Using Trees},
  intype    = {presented at the},
  booktitle = {Proc. Int. Conf. Commun.},
  year      = {1997}
}

@mastersthesis{karnik1999tcp,
  author  = {A. Karnik},
  title   = {Performance of {TCP} Congestion Control with Rate
             Feedback: {TCP/ABR} and Rate Adaptive {TCP/IP}},
  school  = {Indian Institute of Science},
  type    = {M. Eng. thesis},
  address = {Bangalore, India},
  month   = jan,
  year    = {1999}
}

@phdthesis{li2000networks,
  author  = {Q. Li},
  title   = {Delay Characterization and Performance Control of
             Wide-area Networks},
  school  = {Univ. of Delaware},
  address = {Newark},
  month   = may,
  year    = {2000}
}

@article{ccsds1999coding,
  author  = {{Consulative Committee for Space Data Systems (CCSDS)}},
  title   = {Telemetry Channel Coding},
  journal = {Blue Book},
  number  = {4},
  year    = {1999}
}

@inproceedings{dollar2009pedestrian,
  author    = {Dollar, Piotr and Wojek, Christian and Schiele, Bernt and Perona, Pietro},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  title     = {Pedestrian detection: A benchmark},
  year      = {2009},
  month     = jun,
  pages     = {304-311},
  abstract  = {Pedestrian detection is a key problem in computer vision, with several applications including robotics, surveillance and automotive safety. Much of the progress of the past few years has been driven by the availability of challenging public datasets. To continue the rapid rate of innovation, we introduce the Caltech Pedestrian Dataset, which is two orders of magnitude larger than existing datasets. The dataset contains richly annotated video, recorded from a moving vehicle, with challenging images of low resolution and frequently occluded people. We propose improved evaluation metrics, demonstrating that commonly used per-window measures are flawed and can fail to predict performance on full images. We also benchmark several promising detection systems, providing an overview of state-of-the-art performance and a direct, unbiased comparison of existing methods. Finally, by analyzing common failure cases, we help identify future research directions for the field.},
  keywords  = {Computer vision;Application software;Robot vision systems;Surveillance;Automotive engineering;Safety;Technological innovation;Vehicles;Image resolution;Failure analysis},
  doi       = {10.1109/CVPR.2009.5206631},
  issn      = {1063-6919}
}

@inproceedings{zhang2017citypersons,
  author = {Zhang, Shanshan and Benenson, Rodrigo and Schiele, Bernt},
  year   = {2017},
  month  = jul,
  pages  = {4457-4465},
  title  = {CityPersons: A Diverse Dataset for Pedestrian Detection},
  doi    = {10.1109/CVPR.2017.474}
}

@article{li2024ltea,
  author   = {Li, Bo and Huang, Shengbao and Zhong, Guangjin},
  journal  = {IEEE Access},
  title    = {LTEA-YOLO: An Improved YOLOv5s Model for Small Object Detection},
  year     = {2024},
  volume   = {12},
  pages    = {99768-99778},
  keywords = {YOLO;Feature extraction;Transformers;Convolutional neural networks;Data mining;Computational modeling;Training;Object detection;Attention mechanism;lightweight transformer;YOLOv5;small object detection},
  doi      = {10.1109/ACCESS.2024.3429282}
}

@article{han2024small,
  author   = {Han, Guang and Guo, Chenwei and Li, Ziyang and Zhao, Haitao},
  journal  = {IEEE Transactions on Cognitive and Developmental Systems},
  title    = {Small Object Detection Based on Microscale Perception and Enhancement-Location Feature Pyramid},
  year     = {2024},
  pages    = {1--15},
  keywords = {Feature extraction;Object detection;Drones;Prediction algorithms;Transformers;Detectors;Convolution;Small object detection;Drone aerial images;Cascade RCNN;Feature pyramid},
  doi      = {10.1109/TCDS.2024.3397684}
}

@misc{khalili2024sodyolo,
  title         = {SOD-YOLOv8 -- Enhancing YOLOv8 for Small Object Detection in Traffic Scenes},
  author        = {Boshra Khalili and Andrew W. Smyth},
  year          = {2024},
  eprint        = {2408.04786},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2408.04786}
}

@article{shi2024focusdet,
  title    = {{FocusDet}: an efficient object detector for small object},
  abstract = {Abstract The object scale of a small object scene changes greatly, and the object is easily disturbed by a complex background...},
  language = {en},
  number   = {1},
  urldate  = {2024-10-01},
  journal  = {Scientific Reports},
  author   = {Shi, Yanli and Jia, Yi and Zhang, Xianhe},
  month    = may,
  year     = {2024},
  pages    = {10697}
}

@article{wang2023oriented,
  title     = {Highly {Efficient} {Anchor}-{Free} {Oriented} {Small} {Object} {Detection} for {Remote} {Sensing} {Images} via {Periodic} {Pseudo}-{Domain}},
  volume    = {15},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  issn      = {2072-4292},
  url       = {https://www.mdpi.com/2072-4292/15/15/3854},
  doi       = {10.3390/rs15153854},
  abstract  = {With the continuous progress of remote sensing image object detection tasks in recent years, researchers in this ﬁeld have gradually shifted the focus of their research from horizontal object detection to the study of object detection in arbitrary directions. It is worth noting that some properties are different from horizontal object detection during oriented object detection that researchers have yet to notice much. This article presents the design of a straightforward and efﬁcient arbitrary-oriented detection system, leveraging the inherent properties of the orientation task, including the rotation angle and box aspect ratio. In the detection of low aspect ratio objects, the angle is of little importance to the orientation bounding box, and it is even difﬁcult to deﬁne the angle information in extreme categories. Conversely, in the detection of objects with high aspect ratios, the angle information plays a crucial role and can have a decisive impact on the quality of the detection results. By exploiting the aspect ratio of different targets, this letter proposes a ratio-balanced angle loss that allows the model to make a better trade-off between low-aspect ratio objects and high-aspect ratio objects. The rotation angle of each oriented object, which we naturally embed into a two-dimensional Euclidean space for regression, thus avoids an overly redundant design and preserving the topological properties of the circular space. The performance of the UCAS-AOD, HRSC2016, and DLR-3K datasets show that the proposed model in this paper achieves a leading level in terms of both accuracy and speed.},
  language  = {en},
  number    = {15},
  urldate   = {2024-10-01},
  journal   = {Remote Sensing},
  author    = {Wang, Minghui and Li, Qingpeng and Gu, Yunchao and Pan, Junjun},
  month     = aug,
  year      = {2023},
  pages     = {3854}
}

@software{jocher2022yolo5,
  author    = {Glenn Jocher and
               Ayush Chaurasia and
               Alex Stoken and
               Jirka Borovec and
               NanoCode012 and
               Yonghye Kwon and
               Kalen Michael and
               TaoXie and
               Jiacong Fang and
               imyhxy and
               Lorna and
               {Zeng, Yifu} and
               Colin Wong and
               Abhiram V and
               Diego Montes and
               Zhiqiang Wang and
               Cristi Fati and
               Jebastin Nadar and
               Laughing and
               UnglvKitDe and
               Victor Sonck and
               tkianai and
               yxNONG and
               Piotr Skalski and
               Adam Hogan and
               Dhruv Nair and
               Max Strobel and
               Mrinal Jain},
  title     = {{ultralytics/yolov5: v7.0 - YOLOv5 SOTA Realtime 
               Instance Segmentation}},
  month     = nov,
  year      = 2022,
  publisher = {Zenodo},
  version   = {v7.0},
  doi       = {10.5281/zenodo.7347926},
  url       = {https://doi.org/10.5281/zenodo.7347926}
}

@inproceedings{wang2023yolov7,
  title     = {{YOLOv7}: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors},
  author    = {Wang, Chien-Yao and Bochkovskiy, Alexey and Liao, Hong-Yuan Mark},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2023}
}

@inproceedings{he2014spp,
  author    = {He, Kaiming
               and Zhang, Xiangyu
               and Ren, Shaoqing
               and Sun, Jian},
  editor    = {Fleet, David
               and Pajdla, Tomas
               and Schiele, Bernt
               and Tuytelaars, Tinne},
  title     = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
  booktitle = {Computer Vision -- ECCV 2014},
  year      = {2014},
  publisher = {Springer International Publishing},
  address   = {Cham},
  pages     = {346--361},
  abstract  = {Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g. 224{\texttimes}224) input image. This requirement is ``artificial'' and may hurt the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with a more principled pooling strategy, ``spatial pyramid pooling'', to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. By removing the fixed-size limitation, we can improve all CNN-based image classification methods in general. Our SPP-net achieves state-of-the-art accuracy on the datasets of ImageNet 2012, Pascal VOC 2007, and Caltech101.},
  isbn      = {978-3-319-10578-9}
}

@software{jocher2024yolo11,
  author  = {Glenn Jocher and Jing Qiu},
  title   = {Ultralytics YOLO11},
  version = {11.0.0},
  year    = {2024},
  url     = {https://github.com/ultralytics/ultralytics},
  orcid   = {0000-0001-5950-6979, 0000-0002-7603-6750, 0000-0003-3783-7069},
  license = {AGPL-3.0}
}

@article{han2016compression,
  title         = {Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  author        = {Song Han and Huizi Mao and William J. Dally},
  year          = {2016},
  eprint        = {1510.00149},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1510.00149}
}

@article{lin2017binary,
  title   = {Towards accurate binary convolutional neural network},
  author  = {Lin, Xiaofan and Zhao, Cong and Pan, Wei},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@inproceedings{ret2022,
  author    = {P. Ganesh and Y. Chen and Y. Yang and D. Chen and M. Winslett},
  booktitle = {2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  title     = {YOLO-ReT: Towards High Accuracy Real-time Object Detection on Edge GPUs},
  year      = {2022},
  volume    = {},
  issn      = {},
  pages     = {1311-1321},
  abstract  = {Performance of object detection models has been growing rapidly on two major fronts, model accuracy and efficiency. However, in order to map deep neural network (DNN) based object detection models to edge devices, one typically needs to compress such models significantly, thus compromising the model accuracy. In this paper, we propose a novel edge GPU friendly module for multi-scale feature interaction by exploiting missing combinatorial connections between various feature scales in existing state-of-the-art methods. Additionally, we propose a novel transfer learning backbone adoption inspired by the changing translational information flow across various tasks, designed to complement our feature interaction module and together improve both accuracy as well as execution speed on various edge GPU devices available in the market. For instance, YOLO-ReT with MobileNetV2×0.75 backbone runs real-time on Jetson Nano, and achieves 68.75 mAP on Pascal VOC and 34.91 mAP on COCO, beating its peers by 3.05 mAP and 0.91 mAP respectively, while executing faster by 3.05 FPS. Furthermore, introducing our multi-scale feature interaction module in YOLOv4-tiny and YOLOv4-tiny (3l) improves their performance to 41.5 and 48.1 mAP respectively on COCO, outperforming the original versions by 1.3 and 0.9 mAP.},
  keywords  = {performance evaluation;deep learning;computer vision;image edge detection;transfer learning;neural networks;graphics processing units},
  doi       = {10.1109/WACV51458.2022.00138},
  url       = {https://doi.ieeecomputersociety.org/10.1109/WACV51458.2022.00138},
  publisher = {IEEE Computer Society},
  address   = {Los Alamitos, CA, USA},
  month     = {jan}
}

@article{ni2024yolo,
  author         = {Ni, Jianjun and Zhu, Shengjie and Tang, Guangyi and Ke, Chunyan and Wang, Tingting},
  title          = {A Small-Object Detection Model Based on Improved YOLOv8s for UAV Image Scenarios},
  journal        = {Remote Sensing},
  volume         = {16},
  year           = {2024},
  number         = {13},
  article-number = {2465},
  url            = {https://www.mdpi.com/2072-4292/16/13/2465},
  issn           = {2072-4292},
  abstract       = {Small object detection for unmanned aerial vehicle (UAV) image scenarios is a challenging task in the computer vision field. Some problems should be further studied, such as the dense small objects and background noise in high-altitude aerial photography images. To address these issues, an enhanced YOLOv8s-based model for detecting small objects is presented. The proposed model incorporates a parallel multi-scale feature extraction module (PMSE), which enhances the feature extraction capability for small objects by generating adaptive weights with different receptive fields through parallel dilated convolution and deformable convolution, and integrating the generated weight information into shallow feature maps. Then, a scale compensation feature pyramid network (SCFPN) is designed to integrate the spatial feature information derived from the shallow neural network layers with the semantic data extracted from the higher layers of the network, thereby enhancing the network's capacity for representing features. Furthermore, the largest-object detection layer is removed from the original detection layers, and an ultra-small-object detection layer is applied, with the objective of improving the network's detection performance for small objects. Finally, the WIOU loss function is employed to balance high- and low-quality samples in the dataset. The results of the experiments conducted on the two public datasets illustrate that the proposed model can enhance the object detection accuracy in UAV image scenarios.},
  doi            = {10.3390/rs16132465}
}

@article{distill,
  author        = {{Polino}, A. and {Pascanu}, R. and {Alistarh}, D.},
  title         = {{Model compression via distillation and quantization}},
  journal       = {ArXiv e-prints},
  archiveprefix = {arXiv},
  eprint        = {1802.05668},
  keywords      = {Computer Science - Neural and Evolutionary Computing, Computer Science - Learning},
  year          = 2018,
  month         = feb
}

@article{xiuling2024,
  title    = {Starting from the structure: A review of small object detection based on deep learning},
  journal  = {Image and Vision Computing},
  volume   = {146},
  pages    = {105054},
  year     = {2024},
  issn     = {0262-8856},
  doi      = {https://doi.org/10.1016/j.imavis.2024.105054},
  url      = {https://www.sciencedirect.com/science/article/pii/S0262885624001586},
  author   = {Zheng Xiuling and Wang Huijuan and Shang Yu and Chen Gang and Zou Suhua and Yuan Quanbo},
  keywords = {Small object detection, Data augmentation, Feature extraction, Feature fusion, Unsupervised, End-to-end, Transfer learning, Anchor-free},
  abstract = {Object detection, as one of the most fundamental and essential tasks in the field of computer vision, has been the focus of unremitting efforts by researchers, who are committed to modifying the neural network structure in order to improve the accuracy of object detection and expedite task execution. As the application scope continues to expand, small object detection has gradually emerged as a crucial branch in the field of object detection. In this paper, the development history of object detection algorithms is introduced, the concept of small objects is introduced, and the current problems and challenges faced by small object detection are outlined. In this paper, the network structure is disassembled from a macroscopic point of view, and improved algorithms such as enhanced data augmentation, improved feature extraction, superior feature fusion, and refined loss functions are described in detail. Furthermore, the paper explores a series of emerging and improved algorithms for small object detection. It encompasses the introduction of advanced strategies such as unsupervised learning, end-to-end training, density cropping, transfer learning, and anchor-free approaches. The paper provides a comprehensive list of commonly used general-purpose datasets and domain-specific datasets for small object detection tasks, offering performance comparisons of the mentioned improved algorithms. In conclusion, the paper summarizes and provides an outlook on current small object detection algorithms, furnishing the reader with a thorough understanding of the field and insights into future directions.}
}

@article{woebbecke1995color,
  title     = {Color indices for weed identification under various soil, residue, and lighting conditions},
  abstract  = {Color slide images of weeds among various soils and residues were digitized and analyzed for red, green, and blue (RGB) color content. Red, green, and blue chromatic coordinates (rgb) of plants were very different from those of background soils and residue. To distinguish living plant material from a nonplant background, several indices of chromatic coordinates were studied, tested, and were successful in identifying weeds. The indices included r-g, g-b, (g-b)||r-g|, and 2g-r-b. A modified hue was also used to distinguish weeds from non-plant surfaces. The modified hue, 2g-r-b index, and the green chromatic coordinate distinguished weeds from a nonplant background (0.05 level of significance) better than other indices. However, the modified hue was the most computationally intense. These indices worked well for both nonshaded and shaded sunlit conditions. These indices could be used for sensor design for detecting weeds for spot spraying control.},
  author    = {Woebbecke, {D. M.} and Meyer, {G. E.} and {Von Bargen}, K. and Mortensen, {D. A.}},
  year      = {1995},
  month     = jan,
  language  = {English (US)},
  volume    = {38},
  pages     = {259--269},
  journal   = {Transactions of the American Society of Agricultural Engineers},
  issn      = {0001-2351},
  publisher = {American Society of Agricultural and Biological Engineers},
  number    = {1}
}

@misc{lee2023efficient,
  title         = {An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection},
  author        = {Youngwan Lee and Joong-won Hwang and Sangrok Lee and Yuseok Bae and Jongyoul Park},
  year          = {2019},
  eprint        = {1904.09730},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1904.09730}
}

@misc{tang2022ghostnet,
  title         = {GhostNetV2: Enhance Cheap Operation with Long-Range Attention},
  author        = {Yehui Tang and Kai Han and Jianyuan Guo and Chang Xu and Chao Xu and Yunhe Wang},
  year          = {2022},
  eprint        = {2211.12905},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2211.12905}
}

@misc{micikevicius2018mixed,
  title         = {Mixed Precision Training},
  author        = {Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
  year          = {2018},
  eprint        = {1710.03740},
  archiveprefix = {arXiv},
  primaryclass  = {cs.AI},
  url           = {https://arxiv.org/abs/1710.03740}
}

@misc{nvidia2022orin,
  title        = {Develop for All Six NVIDIA Jetson Orin Modules with the Power of One Developer Kit},
  author       = {Suhas Hariharapura Sheshadri},
  year         = {2022},
  url          = {https://developer.nvidia.com/blog/develop-for-all-six-nvidia-jetson-orin-modules-with-the-power-of-one-developer-kit/},
  note         = {Accessed: 2024-10-03},
  organization = {NVIDIA Corporation}
}

@article{wen2023underwater,
  author         = {Wen, Ge and Li, Shaobao and Liu, Fucai and Luo, Xiaoyuan and Er, Meng-Joo and Mahmud, Mufti and Wu, Tao},
  title          = {YOLOv5s-CA: A Modified YOLOv5s Network with Coordinate Attention for Underwater Target Detection},
  journal        = {Sensors},
  volume         = {23},
  year           = {2023},
  number         = {7},
  article-number = {3367},
  url            = {https://www.mdpi.com/1424-8220/23/7/3367},
  pubmedid       = {37050427},
  issn           = {1424-8220},
  abstract       = {Underwater target detection techniques have been extensively applied to underwater vehicles for marine surveillance, aquaculture, and rescue applications. However, due to complex underwater environments and insufficient training samples, the existing underwater target recognition algorithm accuracy is still unsatisfactory. A long-term effort is essential to improving underwater target detection accuracy. To achieve this goal, in this work, we propose a modified YOLOv5s network, called YOLOv5s-CA network, by embedding a Coordinate Attention (CA) module and a Squeeze-and-Excitation (SE) module, aiming to concentrate more computing power on the target to improve detection accuracy. Based on the existing YOLOv5s network, the number of bottlenecks in the first C3 module was increased from one to three to improve the performance of shallow feature extraction. The CA module was embedded into the C3 modules to improve the attention power focused on the target. The SE layer was added to the output of the C3 modules to strengthen model attention. Experiments on the data of the 2019 China Underwater Robot Competition were conducted, and the results demonstrate that the mean Average Precision (mAP) of the modified YOLOv5s network was increased by 2.4%.},
  doi            = {10.3390/s23073367}
}

@article{li2022thangka,
  author   = {Li, Yubo and Fan, Yao and Wang, Shuaishuai and Bai, Jianxian and Li, Keying},
  journal  = {IEEE Access},
  title    = {Application of YOLOv5 Based on Attention Mechanism and Receptive Field in Identifying Defects of Thangka Images},
  year     = {2022},
  volume   = {10},
  number   = {},
  pages    = {81597-81611},
  abstract = {Aiming at the problems of target detection network in the defect detection field of thangka images with complex background colors, such as poor small target detection effect, insufficient feature information extraction, prone to error detection and leak detection, and low accuracy of defect detection, this paper proposed the YOLOv5 defect detection algorithm combining attention mechanism and receptive field. First of all, the Backbone network is used for feature extraction, integrating attention mechanism to represent different features, so that the network can fully extract the texture and semantic features of the defect area, and the extracted features are weighted and fused to reduce information loss. Secondly, a weighted fusion of features of different dimensions is transferred by the Neck network, and the combination of FPN and PAN is used to realize the fusion of semantic features and texture features of different layers and to locate the defect target more accurately. Finally, while replacing the GIoU loss function with CIoU, the receptive field is added to the network, so that the algorithm uses a four-channel detection mechanism to expand the detection range of receptive fields, and fuses semantic information between different network layers, so as to achieve fast location and more refined processing of small targets. The experimental results show that compared with the original YOLOv5 network, the detection accuracy of YOLOV5-scSE and YOLOV5-CA networks proposed in this paper has improved by 8.71 percentage points and 10.97 percentage points respectively, and the verification index has been significantly improved. It can quickly and more accurately identify and locate the location of the defect area and has a stronger ability to generalize the defect category, which greatly improves the accuracy of thangka image defect detection.},
  keywords = {Loss measurement;Deep learning;Insulators;Feature extraction;Convolutional neural networks;Semantics;Predictive models;Object detection;Leak detection;Information retrieval;Image color analysis;YOLOv5;defective region;tangka dataset;deep learning;scSE;CA},
  doi      = {10.1109/ACCESS.2022.3195176},
  issn     = {2169-3536},
  month    = {}
}

@inproceedings{mobileone,
  title     = {MobileOne: An Improved One millisecond Mobile Backbone},
  booktitle = {CVPR},
  author    = {Pavan Kumar Anasosalu Vasu and James Gabriel and Jeff Zhu and Oncel Tuzel and Anurag Ranjan},
  year      = {2023},
  url       = {https://arxiv.org/abs/2206.04040}
}

@inproceedings{orin,
  author = {Youvan, Douglas},
  year   = {2024},
  month  = {06},
  pages  = {},
  title  = {Developing and Deploying AI Applications on NVIDIA Jetson Orin NX: A Comprehensive Guide},
  doi    = {10.13140/RG.2.2.15641.43363}
}

@misc{liu2023edgeyolo,
  title         = {EdgeYOLO: An Edge-Real-Time Object Detector},
  author        = {Shihan Liu and Junlin Zha and Jian Sun and Zhuo Li and Gang Wang},
  year          = {2023},
  eprint        = {2302.07483},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2302.07483}
}

@article{han2022context,
  title   = {A context-scale-aware detector and a new benchmark for remote sensing small weak object detection in unmanned aerial vehicle images},
  journal = {Int. J. Appl. Earth Obs. Geoinformation},
  volume  = {112},
  pages   = {102966},
  year    = {2022},
  issn    = {1569-8432},
  doi     = {https://doi.org/10.1016/j.jag.2022.102966},
  author  = {Wei Han and Jun Li and Sheng Wang and Yi Wang and Jining Yan and Runyu Fan and Xiaohan Zhang and Lizhe Wang}
}

@article{cheng2023largescale,
  author   = {Cheng, Gong and Yuan, Xiang and Yao, Xiwen and Yan, Kebing and Zeng, Qinghua and Xie, Xingxing and Han, Junwei},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Towards Large-Scale Small Object Detection: Survey and Benchmarks},
  year     = {2023},
  volume   = {45},
  number   = {11},
  pages    = {13467-13488},
  keywords = {Object detection;Surveys;Feature extraction;Benchmark testing;Deep learning;Task analysis;Pedestrians;Benchmark;convolutional neural networks;deep learning;object detection;small object detection},
  doi      = {10.1109/TPAMI.2023.3290594}
}

@inproceedings{xia2018dota,
  author    = {Xia, Gui-Song and Bai, Xiang and Ding, Jian and Zhu, Zhen and Belongie, Serge and Luo, Jiebo and Datcu, Mihai and Pelillo, Marcello and Zhang, Liangpei},
  title     = {DOTA: A Large-Scale Dataset for Object Detection in Aerial Images},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  month     = {June},
  year      = {2018}
}

@inproceedings{zzl,
  author = {Zhu, Zhe and Liang, Dun and Zhang, Song-Hai and Huang, Xiaolei and Li, Baoli and Hu, Shimin},
  year   = {2016},
  month  = {06},
  pages  = {2110-2118},
  title  = {Traffic-Sign Detection and Classification in the Wild},
  doi    = {10.1109/CVPR.2016.232}
}
@inproceedings{lee2021,
  title     = {Layer-adaptive Sparsity for the Magnitude-based Pruning},
  author    = {Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},
  booktitle = {International Conference on Learning Representations},
  year      = {2021},
  url       = {https://openreview.net/forum?id=H6ATjJ0TKdf}
}

@misc{liu2023evit,
  title         = {EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention},
  author        = {Xinyu Liu and Houwen Peng and Ningxin Zheng and Yuqing Yang and Han Hu and Yixuan Yuan},
  year          = {2023},
  eprint        = {2305.07027},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2305.07027}
}
@misc{faster,
  title         = {Run, Don't Walk: Chasing Higher FLOPS for Faster Neural Networks},
  author        = {Jierun Chen and Shiu-hong Kao and Hao He and Weipeng Zhuo and Song Wen and Chul-Ho Lee and S. -H. Gary Chan},
  year          = {2023},
  eprint        = {2303.03667},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2303.03667}
}
@misc{mobilenetv4_bottleneck,
  title         = {Rethinking Bottleneck Structure for Efficient Mobile Network Design},
  author        = {Zhou Daquan and Qibin Hou and Yunpeng Chen and Jiashi Feng and Shuicheng Yan},
  year          = {2020},
  eprint        = {2007.02269},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2007.02269}
}
@misc{ghost,
  title         = {GhostNet: More Features from Cheap Operations},
  author        = {Kai Han and Yunhe Wang and Qi Tian and Jianyuan Guo and Chunjing Xu and Chang Xu},
  year          = {2020},
  eprint        = {1911.11907},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1911.11907}
}


@article{electronics13163269,
  author         = {Qiu, Xiaoyang and Chen, Yajun and Cai, Wenhao and Niu, Meiqi and Li, Jianying},
  title          = {LD-YOLOv10: A Lightweight Target Detection Algorithm for Drone Scenarios Based on YOLOv10},
  journal        = {Electronics},
  volume         = {13},
  year           = {2024},
  number         = {16},
  article-number = {3269},
  url            = {https://www.mdpi.com/2079-9292/13/16/3269},
  issn           = {2079-9292},
  abstract       = {Due to the limited computing resources and storage capacity of edge detection devices, efficient detection algorithms are typically required to meet real-time and accuracy requirements. Existing detectors often require a large number of parameters and high computational power to improve accuracy, which reduces detection speed and performance on low-power devices. To reduce computational load and enhance detection performance on edge devices, we propose a lightweight drone target detection algorithm, LD-YOLOv10. Firstly, we design a novel lightweight feature extraction structure called RGELAN, which utilizes re-parameterized convolutions and the newly designed Conv-Tiny as the computational structure to reduce the computational burden of feature extraction. The AIFI module was introduced, utilizing its multi-head attention mechanism to enhance the expression of semantic information. We construct the DR-PAN Neck structure, which obtains weak features of small targets with minimal computational load. Wise-IoU and EIoU are combined as new bounding box regression loss functions to adjust the competition between anchor boxes of different quality and the sensitivity of anchor box aspect ratios, providing a more intelligent gradient allocation strategy. Extensive experiments on the VisdroneDET-2021 and UAVDT datasets show that LD-YOLOv10 reduces the number of parameters by 62.4% while achieving a slight increase in accuracy and has a faster detection speed compared to other lightweight algorithms. When deployed on the low-power NVIDIA Jetson Orin Nano device, LD-YOLOv10 achieves a detection speed of 25 FPS.},
  doi            = {10.3390/electronics13163269}
}

@misc{efficient,
	title={EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention}, 
	author={Xinyu Liu and Houwen Peng and Ningxin Zheng and Yuqing Yang and Han Hu and Yixuan Yuan},
	year={2023},
	eprint={2305.07027},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2305.07027}, 
}

@Article{sppfcspc,
	AUTHOR = {Zhou, Xue and Wei, Wei and Huang, Zhen and Su, Zhiwei},
	TITLE = {Study on the Detection Mechanism of Multi-Class Foreign Fiber under Semi-Supervised Learning},
	JOURNAL = {Applied Sciences},
	VOLUME = {14},
	YEAR = {2024},
	NUMBER = {12},
	ARTICLE-NUMBER = {5246},
	URL = {https://www.mdpi.com/2076-3417/14/12/5246},
	ISSN = {2076-3417},
	ABSTRACT = {Foreign fibers directly impact the quality of raw cotton, affecting the prices of textile products and the economic efficiency of cotton textile enterprises. The accurate differentiation and labeling of foreign fibers require domain-specific knowledge, and labeling scattered cotton foreign fibers in images consumes substantial time and labor costs. In this study, we propose a semi-supervised foreign fiber detection approach that uses unlabeled image information and a small amount of labeled data for model training. Our proposed method, Efficient YOLOv5-cotton, introduces CBAM to address the issue of the missed detection and false detection of small-sized cotton foreign fibers against complex backgrounds. Second, the algorithm designs a multiscale feature information extraction network, SPPFCSPC, which improves its ability to generalize to fibers of different shapes. Lastly, to reduce the increased network parameters and computational complexity introduced by the SPPFCSPC module, we replace the C3 layer with the C3Ghost module. We evaluate Efficient YOLOv5 for detecting various types of foreign fibers. The results demonstrate that the improved Efficient YOLOv5-cotton achieves a 1.6% increase in mAP@0.5 (mean average precision) compared with the original Efficient YOLOv5 and reduces model parameters by 10% compared to the original Efficient YOLOv5 with SPPFCSPC. Our experiments show that our proposed method enhances the accuracy of foreign fiber detection using Efficient YOLOv5-cotton and considers the trade-off between the model size and computational cost.},
	DOI = {10.3390/app14125246}
}

@inproceedings{lin2014microsoft,
  title        = {Microsoft COCO: Common objects in context},
  author       = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle    = {Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages        = {740--755},
  year         = {2014},
  organization = {Springer}
}
@inproceedings{nowak2018relaxation,
  title        = {On the relaxation of the 8-bit quantization constraint on the weights of neural networks},
  author       = {Nowak, Grzegorz and Cieslewski, Tomasz and Jiao, Ran and Gavves, Efstratios and Cornea, Raymond and Sebe, Nicu},
  booktitle    = {2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages        = {969--977},
  year         = {2018},
  organization = {IEEE}
}





